{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download() #check that everything is installed on the PC and uptodate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggles\n",
    "# stack_to_save is for saving the stack model classifier or Training Model once it is run\n",
    "# stack_to_load is for using a prior saved stack classifier or Training Model instead of training it again\n",
    "\n",
    "stack_to_save = 1 # must be 0 or 1\n",
    "stack_to_load = 0 # must be 0 or 1, cant be same as stack_to_save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# pandas dataframes to hold the tweets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# use nltk for the natural language processing\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "# deep learning\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Activation\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# # filter out all warnings, leave this commented out, interested to see the warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use random_state of 0 to stay consistent when training models are run\n",
    "# and therefore able to detect changes in the output\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check on the panda version and its dependencies\n",
    "# i run this from time to time to ensure all is up to date\n",
    "pd.__version__\n",
    "#pd.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the training data into a dataframe, kaggle https://www.kaggle.com/kazanova/sentiment140\n",
    "# add the column names\n",
    "df_train = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\",header=None)\n",
    "df_train.columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# exploring the training data\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      "target    1600000 non-null int64\n",
      "id        1600000 non-null int64\n",
      "date      1600000 non-null object\n",
      "flag      1600000 non-null object\n",
      "user      1600000 non-null object\n",
      "text      1600000 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# there is no missing data\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target: 0 = negative, 4 = positive\n",
    "df_train['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seems there is exactly 800k positive and 800k negative tweets, total tweets 1.6m\n",
    "# i will change the 4 into a 1 later before i train the model\n",
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set up training for bag of words as a first try\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a first step, i will need to clean the text\n",
    "# step 1, clean each tweet to get rid of labels, @names, urls and put into lower case\n",
    "# Step 2, filter out the stopwords and stem the rest\n",
    "# step 3, convert the words to numbers for use in the training model\n",
    "# step 4, pull out X and y for the model and pad them to make them all the same length\n",
    "# step 5, split the data into training and testing date\n",
    "# step 6, build the model\n",
    "# step 7, train the model\n",
    "# step 8, evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "1        is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      "2                              @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
      "3                                                                        my whole body feels itchy and like its on fire \n",
      "4        @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 140):\n",
    "    print(df_train['text'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "\n",
    "text_cleaned = []\n",
    "\n",
    "for i in range(len(df_train.index)):\n",
    "    tweet_to_clean = df_train['text'][i]\n",
    "    tweet_to_clean = re.sub(r'#([^\\s]+)', r'\\1', tweet_to_clean) # all #hashtag goes to hashtag\n",
    "    tweet_to_clean = re.sub('@[^\\s]+','', tweet_to_clean) # all @name deleted\n",
    "    tweet_to_clean = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet_to_clean) # all urls deleted\n",
    "    tweet_to_clean = re.sub('[^a-zA-z0-9\\s]','',tweet_to_clean) # punctuation and special characters now cleared\n",
    "    tweet_to_clean = re.sub('[\\s]+', ' ', tweet_to_clean) # converts all the resulting whitespace to one space only\n",
    "    tweet_to_clean = tweet_to_clean.lower() # convert all to lower case\n",
    "    \n",
    "    text_cleaned.append(tweet_to_clean)\n",
    "\n",
    "df_train['text_cleaned'] = text_cleaned # new dataframe column with the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                      awww thats a bummer you shoulda got david carr of third day to do it d\n",
      "1    is upset that he cant update his facebook by texting it and might cry as a result school today also blah\n",
      "2                                i dived many times for the ball managed to save 50 the rest go out of bounds\n",
      "3                                                             my whole body feels itchy and like its on fire \n",
      "4                      no its not behaving at all im mad why am i here because i cant see you all over there \n",
      "Name: text_cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 140):\n",
    "    print (df_train['text_cleaned'][0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target__</th>\n",
       "      <th>id__</th>\n",
       "      <th>date__</th>\n",
       "      <th>flag__</th>\n",
       "      <th>user__</th>\n",
       "      <th>text__</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>awww thats a bummer you shoulda got david car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>i dived many times for the ball managed to sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no its not behaving at all im mad why am i he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target__        id__                        date__    flag__  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "            user__                                             text__  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0   awww thats a bummer you shoulda got david car...  \n",
       "1  is upset that he cant update his facebook by t...  \n",
       "2   i dived many times for the ball managed to sa...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4   no its not behaving at all im mad why am i he...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i will also need to change the column headers in case they conflict with the feature columns later\n",
    "# i.e. if a feature word is 'text' or 'date' or 'flag' etc\n",
    "# add'__' to each column header\n",
    "\n",
    "for i in range(6):\n",
    "    df_train.rename(columns={df_train.columns[i]: df_train.columns[i]+'__'}, inplace=True)\n",
    "    \n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filtStem = []\n",
    "\n",
    "for i in range(len(df_train.index)):\n",
    "    tweets_toFilter = df_train['text_cleaned'][i]\n",
    "    words = word_tokenize(tweets_toFilter)\n",
    "\n",
    "    new_tweet = []\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            new_tweet.append(stemmer.stem(w))\n",
    "    tweets_filtStem.append(new_tweet)\n",
    "\n",
    "df_train['text_stopwordsRemoved'] = tweets_filtStem # new dataframe column with the filtered text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                            [awww, that, bummer, shoulda, got, david, carr, third, day]\n",
      "1    [upset, cant, updat, facebook, text, might, cri, result, school, today, also, blah]\n",
      "2                             [dive, mani, time, ball, manag, save, 50, rest, go, bound]\n",
      "3                                                 [whole, bodi, feel, itchi, like, fire]\n",
      "4                                                            [behav, im, mad, cant, see]\n",
      "Name: text_stopwordsRemoved, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 140):\n",
    "    print(df_train['text_stopwordsRemoved'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11697422"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text_stopwordsRemoved'].map(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "\n",
    "max_features = 5000 # keep the 5000 most common words to use as features\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(df_train['text_stopwordsRemoved'].values)\n",
    "df_train['tokenized'] = tokenizer.texts_to_sequences(df_train['text_stopwordsRemoved'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and save the tokenizer for use when loading the model to use later\n",
    "\n",
    "with open('tokenizer_keras_twitter.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4\n",
    "\n",
    "max_len = 70 # this should cover all tweets\n",
    "\n",
    "X = df_train['tokenized']\n",
    "X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "df_train['model_target'] = df_train['target__'].replace([4],[1])\n",
    "\n",
    "y = df_train['model_target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,  399,   51, 1081, 3056,\n",
       "           9,  720, 1671,    4]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0]), X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200000, 70), (1200000,), (400000, 70), (400000,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 5\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)\n",
    "X_train.shape,y_train.shape, X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6\n",
    "\n",
    "#Define Keras sequential classifier\n",
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 120 batch size, 5 epochs gets 52% accuracy\n",
    "\n",
    "# batch_size = 120\n",
    "# epochs = 5\n",
    "\n",
    "# model.add(Dense(max_features, activation='relu', input_dim=X.shape[1]))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "# from the Keras documentation\n",
    "# gets 79% accuracy with 120 batch and 5 epochs.  32 batch = 79%, 10 epochs with 120 batch gets 79%\n",
    "\n",
    "embed_dim = 128\n",
    "#lstm_out = 20\n",
    "batch_size = 120\n",
    "epochs = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convolution1D model - enhanced\n",
    "# # gets 79% accuracy with 34 batch and 5 epochs\n",
    "\n",
    "# # set parameters:\n",
    "# max_features = max_features\n",
    "# maxlen = maxlen\n",
    "# batch_size = 34\n",
    "# embedding_dims = 50\n",
    "# filters = 250\n",
    "# kernel_size = 3\n",
    "# hidden_dims = 250\n",
    "# #hidden_dims = 150\n",
    "\n",
    "# epochs = 5\n",
    "\n",
    "# # we start off with an efficient embedding layer which maps\n",
    "# # our vocab indices into embedding_dims dimensions\n",
    "# model.add(Embedding(max_features,\n",
    "#                     embedding_dims,\n",
    "#                     input_length=maxlen))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # we add a Convolution1D, which will learn filters\n",
    "# # word group filters of size filter_length:\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "\n",
    "# model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "\n",
    "# model.add(MaxPooling1D(3))\n",
    "# model.add(Conv1D(160, kernel_size, activation='relu'))\n",
    "# model.add(Conv1D(160, kernel_size, activation='relu'))\n",
    "\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convolution1D model\n",
    "# # from the Keras documentation\n",
    "# # gets 79% accuracy with 120 batch and 5 epochs\n",
    "\n",
    "# # set parameters:\n",
    "# max_features = max_features\n",
    "# maxlen = maxlen\n",
    "# batch_size = 120\n",
    "# embedding_dims = 50\n",
    "# filters = 250\n",
    "# kernel_size = 3\n",
    "# #hidden_dims = 250\n",
    "# hidden_dims = 150\n",
    "\n",
    "# epochs = 5\n",
    "\n",
    "# # we start off with an efficient embedding layer which maps\n",
    "# # our vocab indices into embedding_dims dimensions\n",
    "# model.add(Embedding(max_features,\n",
    "#                     embedding_dims,\n",
    "#                     input_length=maxlen))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # we add a Convolution1D, which will learn filters\n",
    "# # word group filters of size filter_length:\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# # we use max pooling:\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# # We add a vanilla hidden layer:\n",
    "# model.add(Dense(hidden_dims))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sentiment classification CNN-LSTM\n",
    "# # from the Keras documentation\n",
    "# # gets 70% accuracy with 120 batch and 5 epochs\n",
    "\n",
    "# # Embedding\n",
    "# max_features = max_features\n",
    "# maxlen = max_len\n",
    "# embedding_size = 128\n",
    "\n",
    "# # Convolution\n",
    "# kernel_size = 5\n",
    "# filters = 64\n",
    "# pool_size = 4\n",
    "\n",
    "# # LSTM\n",
    "# lstm_output_size = 70\n",
    "\n",
    "# # Training\n",
    "# batch_size = 120\n",
    "# epochs = 5\n",
    "\n",
    "# model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# model.add(MaxPooling1D(pool_size=pool_size))\n",
    "# model.add(LSTM(lstm_output_size))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fasttext for text classification\n",
    "# # from the Keras documentation\n",
    "# # batch size 32, 5 epochs gets 77% accuracy\n",
    "\n",
    "# # Set parameters:\n",
    "# # ngram_range = 2 will add bi-grams features\n",
    "# ngram_range = 1\n",
    "# max_features = 20000\n",
    "# #maxlen = 400\n",
    "# batch_size = 32\n",
    "# embedding_dims = 50\n",
    "# epochs = 5\n",
    "\n",
    "\n",
    "# # we start off with an efficient embedding layer which maps\n",
    "# # our vocab indices into embedding_dims dimensions\n",
    "# model.add(Embedding(max_features,\n",
    "#                     embedding_dims))\n",
    "\n",
    "# # we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# # of all words in the document\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary report of Keras classifier:\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         640000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 771,713\n",
      "Trainable params: 771,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#summary report of classifier we have just built\n",
    "print(\"Summary report of Keras classifier:\") \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 7\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1200000/1200000 [==============================] - 1023s 853us/step - loss: 0.4693 - accuracy: 0.7756\n",
      "Epoch 2/5\n",
      "1200000/1200000 [==============================] - 1024s 853us/step - loss: 0.4463 - accuracy: 0.7892\n",
      "Epoch 3/5\n",
      "1200000/1200000 [==============================] - 1026s 855us/step - loss: 0.4355 - accuracy: 0.7954\n",
      "Epoch 4/5\n",
      "1200000/1200000 [==============================] - 1025s 854us/step - loss: 0.4269 - accuracy: 0.8005\n",
      "Epoch 5/5\n",
      "1200000/1200000 [==============================] - 1024s 853us/step - loss: 0.4190 - accuracy: 0.8052\n"
     ]
    }
   ],
   "source": [
    "keras_result = model.fit(X_train, y_train, epochs = epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000/400000 [==============================] - 85s 213us/step\n",
      "\n",
      "ACCURACY: 0.7938525080680847\n",
      "LOSS: 0.4415608602821827\n"
     ]
    }
   ],
   "source": [
    "# Step 8\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print()\n",
    "print(\"ACCURACY:\",score[1])\n",
    "print(\"LOSS:\",score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the keras model\n",
      "keras model saved\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "\n",
    "if stack_to_save == 1:\n",
    "    print('saving the keras model')\n",
    "    model.save('keras_twitter_model.h5')\n",
    "    print('keras model saved')\n",
    "else:\n",
    "    print('model not saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
